\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{url}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=blue,
    citecolor=blue
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{IEEE-CIS Fraud Detection Challenge \\
    \large\textit{A Comparative Study of Binary Classification}
}

\author{
    \IEEEauthorblockN{Khashayar Zardoui}
    \IEEEauthorblockA{\textit{Dept. Computer Science \& Software Engineering} \\
    \textit{Concordia University}\\
        Montreal, Canada \\
        khashayar.zardoui@mail.concordia.ca}
    {\footnotesize ID: 40052568}
    \and
    \IEEEauthorblockN{Paolo Junior Angeloni}
    \IEEEauthorblockA{\textit{Dept. Computer Science \& Software Engineering} \\
    \textit{Concordia University}\\
        Montreal, Canada \\
        p\_ange@live.concordia.ca}
    {\footnotesize ID: 25976944}
}

\pagestyle{plain}
\maketitle
\begin{abstract}
    The objective of this project was to develop a machine learning pipeline capable of identifying fraudulent credit card transactions within the IEEE-CIS Fraud Detection dataset \cite{b1}. Given the extreme class imbalance (3.5\% fraud), we utilized the \textit{Area Under the Receiver Operating Characteristic Curve} (AUC-ROC) \cite{b2} as the primary metric to evaluate the models' ability to distinguish between classes. We implemented and compared LinearSVC, Decision Tree, and XGBoost classifiers. Our findings demonstrate that the XGBoost ensemble significantly outperformed the single-learner architectures, achieving a validation AUC of 0.963 compared to 0.848 for the Decision Tree and 0.815 for the LinearSVC.
\end{abstract}

\section{Exploratory Data Analysis (EDA)}
    \subsection{Data Structure Inspection}
        \begin{enumerate}
            \item \par
                Before any data transformation, we observed the \texttt{train} and \texttt{test} datasets had a mixture of \texttt{float64}, \texttt{int64} and \texttt{object} types
            \item missing values \par
                description goes here...
            \item target balance \par
                description goes here...
        \end{enumerate}
\subsection{Statistical Summary \& Visualizations}
        \begin{figure}[H]
            \centering
            % \includegraphics[width=0.48\textwidth]{throughput_bufferChart.png}
            \caption{some image here}
            % \label{fig:some}
        \end{figure}
        \begin{table}[H]
            \centering
            \caption{some stats...}
            \begin{tabular}{|l|c|}
            \hline
            \textbf{Metric } & \textbf{Value} \\
            \hline
                one & ... \\
                two (\%) & ... \\
                three & ... \\
                four & ... \\
            \hline
            \end{tabular}
            % \label{tab:first_stats_resnet18}
        \end{table}
\subsection{Findings \& Hypotheses}
    ...
    \par\vspace{1em}
    ...

\section{Data Pre-Processing \& Cleaning}
    \subsection{Removal of Noisy and Empty Features \cite{b3}}
       We first calculated the percentage of missing values for every column in the training set. Columns exceeding the defined threshold ($60\%$) of missing data were dropped from both the training and test sets to reduce noise. Additionally, we removed non-predictive features, specifically \texttt{TransactionID} (an index) and \texttt{TransactionDT} (a time-delta), to prevent the model from memorizing row identifiers or learning spurious time-based correlations that would not generalize to future data.

    \subsection{Imputation of Missing Values \cite{b4}}
        The remaining missing values were adjusted using the \texttt{SimpleImputer} from Scikit-Learn. We adopted a split strategy based on feature type:
        \begin{itemize}
            \item \textbf{Numerical Features}: Imputed using the \textbf{median} value. This method was chosen over the mean to be more robust against the heavy outliers present in financial transaction amounts.
            \item \textbf{Categorical Features}: Imputed using the \textbf{most frequent} value to preserve the underlying category distribution.
        \end{itemize}
        The imputers were fitted only on the training set and applied to the test set to avoid data leakage.

    \subsection{Encoding Categorical Features}
        To convert categorical variables (e.g., \texttt{card4}, \texttt{ProductCD}) into a machine-readable numeric format, we applied \textbf{Label Encoding}. 
        \par
        A standard label encoder was fitted on the training data. Known categories were mapped to their learned integers, while unknown categories in the test set were assigned a distinct value of \texttt{-1} to prevent errors during prediction.

    \subsection{Feature Normalization \& Scaling \cite{b3}}
        Finally, we applied the (\texttt{StandardScaler}) to all features. This transformation centers the data such that each feature has a mean of 0 and a variance of 1. 
        \par
        This step is critical for the Support Vector Machine (SVM) model, which relies on Euclidean distance and can be heavily biased by features with large magnitudes (e.g., \texttt{TransactionAmt}) dominating those with small ranges (e.g., encoded categories). While Decision Trees are scale-invariant, using scaled data ensures a consistent pipeline for all models without detrimental effects.

\section{Models}
    \par
        Each model required specific configuration and hyperparameter tuning to handle the dataset's size ($590,000+$ rows) and class imbalance ($3.5\%$ fraud / $96.5\%$ legitimate). We split the training data???
    \subsection{Linear Support Vector Machine (LinearSVC) \cite{b6}}
        To handle the large dataset, we applied \textbf{Principal Component Analysis (PCA)} \cite{b9} for feature reduction to 83 components that explain $95\%$ of the data's variance. We then utilized \texttt{GridSearchCV} \cite{b10} to compare two distinct strategies by tuning the following parameters:
        \begin{itemize}
            \item \textbf{Penalty}: Tested \texttt{l2}, which gently shrinks all feature weights to prevent overfitting, against \texttt{l1}, which aggressively sets weak feature weights to zero.
            \item \textbf{Regularization}: Low values ($[0.1, 0.01, 0.001]$) create a "wider margin" between classes, forcing the model to ignore noise and find a simpler, more generalizable boundary and improve convergence speed.
            \item \textbf{Tolerance}: We adjusted the stopping criteria precision using a standard tolerance ($1e^{-4}$) for the L2 models but a slightly looser tolerance ($1e^{-3}$) for the L1 models to ensure the convergence within a reasonable time.
        \end{itemize}

        \subsection{Decision Tree}
            We implemented a Decision Tree as a non-linear baseline, utilizing \texttt{GridSearchCV} \cite{b10} to evaluate 18 candidate structures evaluated via 3-fold cross-validation:
        \begin{itemize}
            \item \textbf{Max Depth}: We compared restricted depths $[10, 20]$ against \texttt{None}, which allows the tree to grow until all leaves are pure (maximum complexity).
            \item \textbf{Min Samples Split}: Tested $[20, 100, 500]$. Higher values force the tree to learn broader patterns by preventing it from creating specific rules for small groups of outliers.
            \item \textbf{Criterion}: \texttt{'gini'} vs. \texttt{'entropy'} to compare splitting strategies based on Gini Impurity versus Information Gain.
        \end{itemize}

        \subsection{Extreme Gradient Boosting (XGBoost) \cite{b8}}
            We utilized the \texttt{XGBClassifier} with the following hyperparameters:
        \begin{itemize}
            \item \textbf{n\_estimators}: Set to $500$. This defines the ensemble size (number of trees), meaning the model corrects its errors sequentially 500 times to refine predictions.
            \item \textbf{Learning Rate}: Set to $0.05$. A lower rate ensures that no single tree dominates the decision, preventing overfitting and leading to a more stable model.
            \item \textbf{Subsample \& Colsample}: Both set to $0.9$. This forces each tree to train on a random $90\%$ of the rows and $90\%$ of the features.
        \end{itemize}

\section{Model Comparison}
    \par
        The LinearSVC provided a baseline AUC of 0.815, but struggled with convergence times and lacked the complexity to model non-linear fraud patterns. Due to the size of the dataset (590,000+ samples), a standard SVM with a non-linear kernel ($O(n^3)$) was computationally infeasible. We opted for a LinearSVC ($O(n)$) to utilize the entire training set. To satisfy the hyperparameter tuning requirement, we tuned the Regularization parameter (C), penalty (L1 vs. L2) and tolerance, instead of the kernel.
        Additionally, we applied Principal Component Analysis (PCA) to the SVM input to reduce dimensionality, which resolved convergence issues and significantly improved training speed.

        The Decision Tree achieved a higher AUC of 0.848, but exhibited signs of overfitting (high training accuracy vs. lower validation precision), confirming that a single tree has high variance.While it achieved a high F1-score of 0.68 on the training set, this dropped to 0.57 on the validation set. Specifically, the Precision for fraud detection fell from 92\% (training) to 77\% (validation), indicating that some of the decision rules learned were specific to the training noise and did not generalize well. Furthermore, the Recall remained low in both sets (0.53 training vs. 0.45 validation), suggesting that a single decision tree lacks the complexity required to capture the full variety of fraudulent patterns in this dataset.

        XGBoost emerged as the superior model, achieving an AUC-ROC of 0.963 and an F1-Score of 0.70. It successfully balanced a high Precision (93\%) with a Recall of 56\%, significantly outperforming the other models in identifying fraud without disrupting legitimate users. While the Decision Tree suffered from overfitting (high variance), XGBoost demonstrated robust generalization.            
        On the Validation set, XGBoost achieved a Precision of 0.93, meaning it generated very few false positives (false alarms), which is critical for maintaining user trust. Moreover, it achieved a Recall of 0.56, capturing the majority of fraud instances. The F1-Score of 0.70 (Validation) significantly outperforms the Decision Tree (0.57) and indicates that the Gradient Boosting method successfully captured complex, non-linear relationships that the simpler models missed.
    \begin{table}[H]
            \centering
            \caption{training metrics comparison}
            \begin{tabular}{|l|c|c|c|}
            \hline
            \textbf{Metric } & \textbf{LinearSVC} & \textbf{Decision Tree} & \textbf{XGBoost} \\
            \hline
                accuray & 0.74 & ... & 0.98 \\
                precision (0, 1) & 0.99, 0.09 & ... & 0.99, 0.97 \\
                recall (0, 1) & 0.74, 0.73 & ... & 1.00, 0.68 \\
                f1-score (0, 1) & 0.85, 0.16 & ... & 0.99, 0.80 \\
            \hline
            \end{tabular}
            \label{tab:train-compare}
    \end{table}

    \begin{table}[H]
        \centering
        \caption{validation metrics comparison}
        \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Metric } & \textbf{LinearSVC} & \textbf{Decision Tree} & \textbf{XGBoost} \\
        \hline
            accuray & 0.74 & 0.97 & 0.98\\
            precision (0, 1) & 0.99, 0.09 & 0.98, 0.77 & 0.98, 0.93 \\
            recall (0, 1) & 0.74, 0.73 & 1.00, 0.45 & 1.00, 0.56 \\
            f1-score (0, 1) & 0.85, 0.16 & 0.99, 0.57 & 0.99, 0.70 \\
        \hline
        \end{tabular}
        \label{tab:val-compare}
    \end{table}

    \begin{table}[H]
        \centering
        \caption{AUC-ROC comparison}
        \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Metric } & \textbf{LinearSVC} & \textbf{Decision Tree} & \textbf{XGBoost} \\
        \hline
            auc-roc & 0.815 & 0.848 & 0.962  \\
        \hline
        \end{tabular}
        \label{tab:auc-compare}
    \end{table}
        
\section{Kaggle Submission}
    Each model's \textit{AUC-ROC} \cite{b2} results were submitted to the Kaggle competition \cite{b1} in \texttt{.csv} format. Each file is a two-column table with \texttt{TransactionID} and \texttt{isFraud} headers, indicating the confidence level that a transaction is fraudulent. Below are each model's score on the private and public test datasets.
    \begin{figure}[H]
        \centering
            \includegraphics[width=0.48\textwidth]{kaggle.png}
            \caption{Kaggle competition submission results}
            \label{fig:kaggle}
    \end{figure}

\par\vspace{1em}
\section*{Acknowledgment}
    We would like to thank Professor Arash Azarfar and Firat Oncel for their guidance and support throughout this project. Large Language Models, like Google's Gemini were used in an educational context to further understand the resources for this research.

\begin{thebibliography}{00}
    \bibitem{b1} IEEE-CIS Fraud Detection, ``kaggle competition overview,'' [Online]. Available: \url{https://www.kaggle.com/competitions/ieee-fraud-detection/overview}. [Accessed: Nov. 20, 2025].
    \bibitem{b2} metrics, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/api/sklearn.metrics.html}. [Accessed: Dec. 02, 2025].
    \bibitem{b3} data preprocessing, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/preprocessing.html}. [Accessed: Nov. 28, 2025].
    \bibitem{b4} data imputation, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/impute.html}. [Accessed: Nov. 28, 2025].
    \bibitem{b5} label encoding, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/preprocessing_targets.html#label-encoding}. [Accessed: Nov. 28, 2025].
    \bibitem{b6} Support Vector Machines, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/svm.html}. [Accessed: Nov. 30, 2025].
    \bibitem{b7} Decision Trees , ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/tree.html}. [Accessed: Nov. 30, 2025].
    \bibitem{b8} XGBoostClassifier, ``XGBoost API Documentation,'' [Online]. Available: \url{https://xgboost.readthedocs.io/en/stable/}. [Accessed: Nov. 30, 2025].
    \bibitem{b9} Principal Component Analysis (PCA), ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/decomposition.html#pca}. [Accessed: Dec. 02, 2025].
    \bibitem{b10} hyperparameter tuning using GridSearchCV, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/grid_search.html#grid-search}. [Accessed: Nov. 30, 2025].
    \bibitem{b11} Numpy, ``Numpy API Documentation,'' [Online]. Available: \url{https://numpy.org/doc/stable/}. [Accessed: Nov. 25, 2025].
    \bibitem{b12} matplotlib, ``Matplotlib API Documentation,'' [Online]. Available: \url{https://matplotlib.org/stable/index.html}. [Accessed: Nov. 25, 2025].
    \bibitem{b13} pandas, ``pandas API Documentation,'' [Online]. Available: \url{https://pandas.pydata.org/docs/}. [Accessed: Nov. 26, 2025].
    \bibitem{b14} seaborn, ``seaborn API Documentation,'' [Online]. Available: \url{https://seaborn.pydata.org/api.html}. [Accessed: Nov. 26, 2025].
\end{thebibliography}
\vspace{12pt}

\end{document}