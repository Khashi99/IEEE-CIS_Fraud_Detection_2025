\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{url}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=blue,
    citecolor=blue
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{IEEE-CIS Fraud Detection Challenge \\
    \large\textit{Comparative Study of SVM and Decision Tree Binary Classification}
}

\author{
    \IEEEauthorblockN{Khashayar Zardoui}
    \IEEEauthorblockA{\textit{Dept. Computer Science \& Software Engineering} \\
    \textit{Concordia University}\\
    Montreal, Canada \\
    khashayar.zardoui@mail.concordia.ca}
    {\footnotesize ID: 40052568}
    \and
    \IEEEauthorblockN{Paolo Junior Angeloni}
    \IEEEauthorblockA{\textit{Dept. Computer Science \& Software Engineering} \\
    \textit{Concordia University}\\
    Montreal, Canada \\
    p\_ange@live.concordia.ca}
    {\footnotesize ID: 25976944}
}

\pagestyle{plain}
\maketitle
\begin{abstract}
abstract: This report evaluates the performance...
\end{abstract}

\section{The Fraud-Detection Pipeline}
    ...
    \begin{enumerate}
        \item Data loading and exploration (EDA)
        \item Removal, Imputation, Label Encoding and Scaling
        \item Model training and performance assessment
    \end{enumerate}

\section{Exploratory Data Analysis}
    \subsection{Data Structure Inspection}
        \begin{enumerate}
            \item \par
                Before any data transformation, we observed the \texttt{train} and \texttt{test} datasets had a mixture of \texttt{float64}, \texttt{int64} and \texttt{object} types
            \item missing values \par
                description goes here...
            \item target balance \par
                description goes here...
        \end{enumerate}
\subsection{Statistical Summary \& Visualizations}
        \begin{figure}[H]
            \centering
            % \includegraphics[width=0.48\textwidth]{throughput_bufferChart.png}
            \caption{some image here}
            % \label{fig:some}
        \end{figure}
        \begin{table}[H]
            \centering
            \caption{some stats...}
            \begin{tabular}{|l|c|}
            \hline
            \textbf{Metric } & \textbf{Value} \\
            \hline
            one & ... \\
            two (\%) & ... \\
            three & ... \\
            four & ... \\
            \hline
            \end{tabular}
            % \label{tab:first_stats_resnet18}
        \end{table}
\subsection{Findings \& Hypotheses}
    ...
    \par\vspace{1em}
    ...

\section{Data Pre-Processing \& Cleaning}
\subsection{Imputation \& Removal}
        \begin{enumerate}
            \item ...
            \item ...
            \item ...
        \end{enumerate}
    \subsection{Normalize \& Scale Features}
        \begin{enumerate}
            \item ...
            \item ...
            \item ...
        \end{enumerate}
    \subsection{Encoding Categorical Features}
        \begin{enumerate}
            \item ...
            \item ...
            \item ...
        \end{enumerate}

\section{Models}
    \text{intro to the models used}
    \subsection{Support Vector Machine (SVM) Classifier}
    Due to the size of the dataset (590,000+ samples), a standard SVM with a non-linear kernel ($O(n^3)$) was computationally infeasible. We opted for a LinearSVC ($O(n)$) to utilize the entire training set. To satisfy the hyperparameter tuning requirement2, we tuned the Regularization parameter (C) and the Loss function (Hinge vs. Squared Hinge) instead of the kernel.
        
    experiment hyperparameters (C, gamma, kernel etc)
    
    cross-validation and validation splits to evaluate performance
        
    results using different hyperparameters
        
    training and test metrics: confusion matrix, precision, recall, F1-score, and accuracy

    \subsection{Decision Tree Classifier}
        \text{experiment hyperparameters (max depth, min samples split, criterion)}
        \text{cross-validation and validation splits to evaluate performance}
        \text{results using different hyperparameters}
        \text{training and test metrics: confusion matrix, precision, recall, F1-score, and accuracy}

\section{Model Comparison}
    \paragraph{discuss similarities \& differences. use table}    
    \begin{table}[H]
            \centering
            \caption{some stats...}
            \begin{tabular}{|l|c|c|}
            \hline
            \textbf{Metric } & \textbf{SVM} & \textbf{Decision Tree} \\
            \hline
            one & ... & ... \\
            two (\%) & ... & ... \\
            three & ... & ... \\
            four & ... & ... \\
            \hline
            \end{tabular}
            % \label{tab:first_stats_resnet18}
    \end{table}

\par\vspace{1em}
\section*{Acknowledgment}
We would like to thank Professor Arash Azarfar and Firat Oncel for their guidance and support throughout this project. Large Language Models, like Google's Gemini were used in an educational context to further understand the resources for this research.

\begin{thebibliography}{00}
    \bibitem{b1} Numpy, ``Numpy API Documentation,'' [Online]. Available: \url{https://numpy.org/doc/stable/}. [Accessed: Nov. 25, 2025].
    \bibitem{b2} matplotlib, ``Matplotlib API Documentation,'' [Online]. Available: \url{https://matplotlib.org/stable/index.html}. [Accessed: Nov. 25, 2025].
    \bibitem{b3} pandas, ``pandas API Documentation,'' [Online]. Available: \url{https://pandas.pydata.org/docs/}. [Accessed: Nov. 26, 2025].
    \bibitem{b4} seaborn, ``seaborn API Documentation,'' [Online]. Available: \url{https://seaborn.pydata.org/api.html}. [Accessed: Nov. 26, 2025].
    \bibitem{b5} data preprocessing, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/preprocessing.html}. [Accessed: Nov. 28, 2025].
    \bibitem{b6} data imputation, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/impute.html}. [Accessed: Nov. 28, 2025].
    \bibitem{b7} label encoding, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/preprocessing_targets.html#label-encoding}. [Accessed: Nov. 28, 2025].
    \bibitem{b8} hyperparameter tuning using GridSearchCV, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/grid_search.html#grid-search}. [Accessed: Nov. 30, 2025].
    \bibitem{b9} Support Vector Machines, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/svm.html}. [Accessed: Nov. 30, 2025].
    \bibitem{b10} Decision Trees , ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/modules/tree.html}. [Accessed: Nov. 30, 2025].
    \bibitem{b11} XGBoostClassifier, ``XGBoost API Documentation,'' [Online]. Available: \url{https://xgboost.readthedocs.io/en/stable/}. [Accessed: Nov. 30, 2025].
    \bibitem{b12} metrics, ``sklearn API Documentation,'' [Online]. Available: \url{https://scikit-learn.org/stable/api/sklearn.metrics.html}. [Accessed: Dec. 02, 2025].

\end{thebibliography}
\vspace{12pt}

\end{document}